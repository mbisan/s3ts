{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from storage.har_datasets import HARTHDataset, UCI_HARDataset, sts_medoids\n",
    "from s3ts.api.dms.har_datasets import LDFDataset, DFDataset\n",
    "from storage.label_mappings import *\n",
    "from s3ts.api.nets.methods import create_model_from_DM, train_model, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'walking',\n",
       " 2: 'running',\n",
       " 3: 'shuffling',\n",
       " 4: 'stairs_up',\n",
       " 5: 'stairs_down',\n",
       " 6: 'standing',\n",
       " 7: 'sitting',\n",
       " 8: 'lying',\n",
       " 13: 'cycking_sit',\n",
       " 14: 'cycling_stand',\n",
       " 130: 'cycling_sit_idle',\n",
       " 140: 'cycling_stand_idle'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HARTH_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = np.zeros(141)\n",
    "label_mapping[1:9] = np.arange(8)\n",
    "label_mapping[13] = 8\n",
    "label_mapping[14] = 9\n",
    "label_mapping[130] = 10\n",
    "label_mapping[140] = 11\n",
    "\n",
    "ds = HARTHDataset(\"./datasets/HARTH/\", wsize=48, normalize=True, label_mapping=label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6225494"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./datasets/HARTH/meds.npz\"):\n",
    "    meds = sts_medoids(ds, n=500)\n",
    "    with open(\"./datasets/HARTH/meds.npz\", \"wb\") as f:\n",
    "        np.save(f, meds)\n",
    "else:\n",
    "    meds = np.load(\"./datasets/HARTH/meds.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dissimilarity frames if available...\n"
     ]
    }
   ],
   "source": [
    "dfds = DFDataset(ds, patterns=meds, w=0.1, dm_transform=None, ram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in np.random.choice(np.arange(len(dfds)), 500):\n",
    "    dm, _, _ = dfds[i]\n",
    "    DM.append(dm)\n",
    "\n",
    "DM = torch.stack(DM)\n",
    "\n",
    "dm_transform = Normalize(mean=DM.mean(dim=[0, 2, 3]), std=DM.std(dim=[0, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfds.dm_transform = dm_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {\n",
    "    \"train\": lambda x: x<6000000,\n",
    "    \"val\": lambda x: (x>=6000000) * (x<6050000),\n",
    "    \"test\": lambda x: x>=6050000\n",
    "}\n",
    "\n",
    "dm = LDFDataset(dfds, data_split=data_split, batch_size=64, random_seed=42, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6225494"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm.ds_train) + len(dm.ds_val) + len(dm.ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 12, 48, 48])\n",
      "Latent shape:  torch.Size([1, 40, 3, 48])\n"
     ]
    }
   ],
   "source": [
    "model = create_model_from_DM(dm, name=None, \n",
    "        dsrc=\"img\", arch=\"cnn\", task=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name       | Type               | Params\n",
      "---------------------------------------------------\n",
      "0  | encoder    | CNN_IMG            | 24.7 K\n",
      "1  | decoder    | LinearDecoder      | 377 K \n",
      "2  | flatten    | Flatten            | 0     \n",
      "3  | softmax    | Softmax            | 0     \n",
      "4  | train_acc  | MulticlassAccuracy | 0     \n",
      "5  | train_f1   | MulticlassF1Score  | 0     \n",
      "6  | val_acc    | MulticlassAccuracy | 0     \n",
      "7  | val_f1     | MulticlassF1Score  | 0     \n",
      "8  | val_auroc  | MulticlassAUROC    | 0     \n",
      "9  | test_acc   | MulticlassAccuracy | 0     \n",
      "10 | test_f1    | MulticlassF1Score  | 0     \n",
      "11 | test_auroc | MulticlassAUROC    | 0     \n",
      "---------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.610     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|‚ñè         | 1643/93275 [01:28<1:22:32, 18.50it/s, v_num=8, train_loss_step=1.920]"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/mbikandi/Documents/s3ts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mbikandi/Documents/s3ts/examples/train_harth.ipynb Celda 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mbikandi/Documents/s3ts/examples/train_harth.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model, data \u001b[39m=\u001b[39m train_model(dm, model, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mbikandi/Documents/s3ts/examples/train_harth.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(data)\n",
      "File \u001b[0;32m~/Documents/s3ts/s3ts/api/nets/methods.py:96\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dm, model, max_epochs, pl_kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m tr\u001b[39m.\u001b[39mfit(model\u001b[39m=\u001b[39mmodel, datamodule\u001b[39m=\u001b[39mdm)\n\u001b[1;32m     95\u001b[0m \u001b[39m# load the best weights\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mload_from_checkpoint(ckpt\u001b[39m.\u001b[39;49mbest_model_path)\n\u001b[1;32m     98\u001b[0m \u001b[39m# run the validation with the final weights\u001b[39;00m\n\u001b[1;32m     99\u001b[0m data \u001b[39m=\u001b[39m tr\u001b[39m.\u001b[39mvalidate(model, datamodule\u001b[39m=\u001b[39mdm)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssstsc/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1543\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1465\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1471\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   1472\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1543\u001b[0m     loaded \u001b[39m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1544\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1545\u001b[0m         checkpoint_path,\n\u001b[1;32m   1546\u001b[0m         map_location,\n\u001b[1;32m   1547\u001b[0m         hparams_file,\n\u001b[1;32m   1548\u001b[0m         strict,\n\u001b[1;32m   1549\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1550\u001b[0m     )\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssstsc/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_checkpoint\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[39mcls\u001b[39m: Union[Type[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m], Type[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningDataModule\u001b[39m\u001b[39m\"\u001b[39m]],\n\u001b[1;32m     56\u001b[0m     checkpoint_path: Union[_PATH, IO],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     61\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpl.LightningDataModule\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     62\u001b[0m     \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m         checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m     65\u001b[0m     \u001b[39m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     checkpoint \u001b[39m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     67\u001b[0m         checkpoint, checkpoint_path\u001b[39m=\u001b[39m(checkpoint_path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(checkpoint_path, (\u001b[39mstr\u001b[39m, Path)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ssstsc/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:51\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     47\u001b[0m         \u001b[39mstr\u001b[39m(path_or_url),\n\u001b[1;32m     48\u001b[0m         map_location\u001b[39m=\u001b[39mmap_location,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 51\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39;49mopen(path_or_url, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(f, map_location\u001b[39m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssstsc/lib/python3.10/site-packages/fsspec/spec.py:1309\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1308\u001b[0m     ac \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mautocommit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_intrans)\n\u001b[0;32m-> 1309\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(\n\u001b[1;32m   1310\u001b[0m         path,\n\u001b[1;32m   1311\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1312\u001b[0m         block_size\u001b[39m=\u001b[39;49mblock_size,\n\u001b[1;32m   1313\u001b[0m         autocommit\u001b[39m=\u001b[39;49mac,\n\u001b[1;32m   1314\u001b[0m         cache_options\u001b[39m=\u001b[39;49mcache_options,\n\u001b[1;32m   1315\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1316\u001b[0m     )\n\u001b[1;32m   1317\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1318\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/miniconda3/envs/ssstsc/lib/python3.10/site-packages/fsspec/implementations/local.py:180\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_mkdir \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 180\u001b[0m \u001b[39mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssstsc/lib/python3.10/site-packages/fsspec/implementations/local.py:298\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 298\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open()\n",
      "File \u001b[0;32m~/miniconda3/envs/ssstsc/lib/python3.10/site-packages/fsspec/implementations/local.py:303\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mclosed:\n\u001b[1;32m    302\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocommit \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode:\n\u001b[0;32m--> 303\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    304\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression:\n\u001b[1;32m    305\u001b[0m             compress \u001b[39m=\u001b[39m compr[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/mbikandi/Documents/s3ts'"
     ]
    }
   ],
   "source": [
    "model, data = train_model(dm, model, max_epochs=2)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssstsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
