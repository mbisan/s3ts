{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/martin/s3ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from storage.har_datasets import STSDataset, StreamingTimeSeries, StreamingTimeSeriesCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'BasicMotions' from cache...\n",
      "(80, 6, 100) (80,) 4\n",
      "(6, 6000) (6000,)\n"
     ]
    }
   ],
   "source": [
    "from s3ts.api.ucr import load_ucr_classification\n",
    "from s3ts.api.ts2sts import finite_random_STS\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "X, Y, mapping = load_ucr_classification(\"BasicMotions\")\n",
    "#X, Y, mapping = load_ucr_classification(\"GunPoint\")\n",
    "print(X.shape, Y.shape, len(np.unique(Y)))\n",
    "\n",
    "STS, SCS = finite_random_STS(X, Y, length=60)\n",
    "print(STS.shape, SCS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = StreamingTimeSeries(STS, SCS, wsize=32, wstride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LSTSDataset(LightningDataModule):\n",
    "\n",
    "    \"\"\" Data module for the experiments. \"\"\"\n",
    "\n",
    "    STS: np.ndarray     # data stream\n",
    "    SCS: np.ndarray     # class stream\n",
    "    DM: np.ndarray      # dissimilarity matrix\n",
    "\n",
    "    data_split: dict[str: np.ndarray]    \n",
    "                        # train / val / test split\n",
    "    batch_size: int     # dataloader batch size\n",
    "\n",
    "    def __init__(self,\n",
    "            stsds: StreamingTimeSeries,    \n",
    "            data_split: dict, batch_size: int, \n",
    "            random_seed: int = 42, \n",
    "            num_workers: int = 1\n",
    "            ) -> None:\n",
    "\n",
    "        # save parameters as attributes\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.random_seed = random_seed\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.stsds = stsds\n",
    "        self.wdw_len = self.stsds.wsize\n",
    "        self.wdw_str = self.stsds.wstride\n",
    "        self.sts_str = False\n",
    "\n",
    "        # gather dataset info   \n",
    "        self.n_dims = self.stsds.STS.shape[1]\n",
    "        self.n_classes = len(np.unique(self.stsds.SCS))\n",
    "\n",
    "        # convert to tensors\n",
    "        if not torch.is_tensor(self.stsds.STS):\n",
    "            self.stsds.STS = torch.from_numpy(self.stsds.STS).to(torch.float32)\n",
    "        if not torch.is_tensor(self.stsds.SCS):\n",
    "            self.stsds.SCS = torch.from_numpy(self.stsds.SCS).to(torch.int64)\n",
    "\n",
    "        train_indices = self.stsds.indices[data_split[\"train\"](self.stsds.indices)]\n",
    "        test_indices = self.stsds.indices[data_split[\"test\"](self.stsds.indices)]\n",
    "        val_indices = self.stsds.indices[data_split[\"val\"](self.stsds.indices)]\n",
    "\n",
    "        self.ds_train = StreamingTimeSeriesCopy(self.stsds, train_indices)\n",
    "        self.ds_test = StreamingTimeSeriesCopy(self.stsds, test_indices)\n",
    "        self.ds_val = StreamingTimeSeriesCopy(self.stsds, val_indices)\n",
    "        \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Returns the training DataLoader. \"\"\"\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers, shuffle=True,\n",
    "            pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Returns the validation DataLoader. \"\"\"\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers, shuffle=False,\n",
    "            pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Returns the test DataLoader. \"\"\"\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers, shuffle=False,\n",
    "            pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Returns the test DataLoader. \"\"\"\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers, shuffle=False,\n",
    "            pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_shuffled = np.arange(ds.indices.shape[0])[:5000]\n",
    "np.random.shuffle(indices_shuffled)\n",
    "\n",
    "data_split = {\n",
    "    \"train\": lambda x: np.isin(x, indices_shuffled),\n",
    "    \"val\": lambda x: np.isin(x, np.arange(ds.indices.shape[0])[5064:5500]),\n",
    "    \"test\": lambda x: np.isin(x, np.arange(ds.indices.shape[0])[5064:]),\n",
    "}\n",
    "\n",
    "dm = LSTSDataset(ds, data_split=data_split, batch_size=32, random_seed=42, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def dtw_compute(dist_tensor: torch.Tensor, grad_tensor: torch.Tensor, w: float) -> None:\n",
    "    for i in range(1, dist_tensor.shape[2]):\n",
    "        for j in range(1, dist_tensor.shape[3]):\n",
    "            # elements has shape (n, k, 3)\n",
    "            elements = torch.stack([w * dist_tensor[:, :, i, j-1], dist_tensor[:, :, i-1, j], w * dist_tensor[:, :, i-1, j-1]], dim=2)\n",
    "\n",
    "            value, id = torch.min(elements, dim=2) # shape (n, k)\n",
    "\n",
    "            dist_tensor[:,:, i, j] += value\n",
    "\n",
    "            grad_tensor[id==0][:, :, i, j] += w * grad_tensor[id==0][:, :, i, j-1]\n",
    "\n",
    "@torch.jit.script\n",
    "def dtw_compute_by_index(dist_tensor: torch.Tensor, grad_tensor: torch.Tensor, w: float, n: int, s: int) -> None:\n",
    "    for i in range(1, dist_tensor.shape[2]):\n",
    "        for j in range(1, dist_tensor.shape[3]):\n",
    "            elements = torch.stack([w * dist_tensor[n, s, i, j-1], dist_tensor[n, s, i-1, j], w * dist_tensor[n, s, i-1, j-1]], dim=0)\n",
    "\n",
    "            value, id = torch.min(elements, dim=0)\n",
    "\n",
    "            dist_tensor[n, s, i, j] += value\n",
    "\n",
    "            grad_tensor[id==0][n, s, i, j] += w * grad_tensor[id==0][n, s, i, j-1]\n",
    "\n",
    "@torch.jit.script\n",
    "def torch_dtw_fast(x: torch.Tensor, y: torch.Tensor, w: float, eps: float = 1e-5):\n",
    "    # shape of x (n, dim, x_len) y (m, dim, y_len)    \n",
    "    # performs convolution-like operation, for each kernel the DF\n",
    "    # (of shape (kernel_size, T)) is computed, then summed across channels\n",
    "    # x has shape (batch, c, time_dimension)\n",
    "\n",
    "    # compute pairwise diffs (squared)\n",
    "    p_diff = x[:,None,:,None,:] - y[None,:,:,:,None] # shape (n, n_kernel, d, Kernel_size, T)\n",
    "    euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "\n",
    "    # compute dtw\n",
    "    DTW = euc_d.clone()\n",
    "    DTW[:,:,0,:] = torch.cumsum(DTW[:,:,0,:], dim=2)\n",
    "    DTW[:,:,:,0] = torch.cumsum(DTW[:,:,:,0], dim=2)\n",
    "\n",
    "    # p_diff contains the partial derivatives of DTW[n, k, i, j] wrt K[k, d, i] (dims (n, k, d, i, j))\n",
    "    p_diff = p_diff / torch.sqrt(euc_d[:,:, None, :, :] + eps)\n",
    "\n",
    "\n",
    "    dtw_compute(DTW, p_diff, w)\n",
    "    # futures : List[torch.jit.Future[None]] = []\n",
    "    # for n in range(DTW.shape[0]):\n",
    "    #     for s in range(DTW.shape[1]):\n",
    "    #         futures.append(torch.jit.fork(dtw_compute_by_index, DTW, p_diff, w, n, s))\n",
    "\n",
    "    # for future in futures:\n",
    "    #     torch.jit.wait(future)\n",
    "\n",
    "    return DTW.sqrt(), p_diff\n",
    "\n",
    "class torch_dtw(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x, y, w):\n",
    "        DTW, p_diff = torch_dtw_fast(x, y, w)\n",
    "        return DTW, p_diff\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        DTW, p_diff = output\n",
    "        ctx.save_for_backward(p_diff)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dtw_grad, p_diff_grad):\n",
    "        p_diff, = ctx.saved_tensors\n",
    "        mult = (p_diff * dtw_grad[:,:,None,:,:])\n",
    "        return mult.mean(dim=(1, 3)), mult.mean(dim=(0, 4)), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTWLayer(torch.nn.Module):\n",
    "    def __init__(self, n_patts, d_patts, l_patts, l_out: int = None, rho: float = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if not l_out is None:\n",
    "            self.l_out = l_patts\n",
    "        else:\n",
    "            self.l_out = l_patts\n",
    "\n",
    "        self.w: torch.float32 = rho ** (1/l_patts)\n",
    "        self.patts = torch.nn.Parameter(torch.randn(n_patts, d_patts, l_patts))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch_dtw.apply(x, self.patts, self.w)[0][:,:,:,self.l_out:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    DTWLayer(8, 6, 16, l_out=16, rho=0.01),\n",
    "    torch.nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2),\n",
    "    torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(in_features=128, out_features=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torchmetrics as tm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import functional as F\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lmodel(LightningModule):\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, num_classes: int, lr: float = 0.001) -> None:\n",
    "\n",
    "        name = f\"Test model for DTW-layer\"\n",
    "\n",
    "        self.n_classes = num_classes\n",
    "        # save parameters as attributes\n",
    "        super().__init__()\n",
    "\n",
    "        # select model architecture class\n",
    "        self.model = model\n",
    "\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "        self.lr = lr\n",
    "        # create metrics\n",
    "        for phase in [\"train\", \"val\", \"test\"]: \n",
    "            self.__setattr__(f\"{phase}_acc\", tm.Accuracy(num_classes=num_classes, task=\"multiclass\"))\n",
    "            self.__setattr__(f\"{phase}_f1\",  tm.F1Score(num_classes=num_classes, task=\"multiclass\"))\n",
    "            if phase != \"train\":\n",
    "                self.__setattr__(f\"{phase}_auroc\", tm.AUROC(num_classes=num_classes, task=\"multiclass\"))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward pass. \"\"\"\n",
    "        x = self.model(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _inner_step(self, batch: dict[str: torch.Tensor], stage: str = None):\n",
    "\n",
    "        \"\"\" Inner step for the training, validation and testing. \"\"\"\n",
    "        output = self.model(batch[\"series\"])\n",
    "        loss = F.cross_entropy(output, batch[\"label\"])\n",
    "\n",
    "        # compute metrics\n",
    "        acc = self.__getattr__(f\"{stage}_acc\")(output, batch[\"label\"])\n",
    "        f1  = self.__getattr__(f\"{stage}_f1\")(output, batch[\"label\"])\n",
    "        if stage != \"train\":\n",
    "            auroc = self.__getattr__(f\"{stage}_auroc\")(output, batch[\"label\"])  \n",
    "\n",
    "        # log loss and metrics\n",
    "        on_step = True if stage == \"train\" else False\n",
    "        self.log(f\"{stage}_loss\", loss, on_epoch=True, on_step=on_step, prog_bar=True, logger=True)\n",
    "\n",
    "        self.log(f\"{stage}_acc\", acc, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        self.log(f\"{stage}_f1\", f1, on_epoch=True, on_step=False, prog_bar=False, logger=True)\n",
    "        if stage != \"train\":\n",
    "            self.log(f\"{stage}_auroc\", auroc, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "\n",
    "        # return loss\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: dict[str: torch.Tensor], batch_idx: int):\n",
    "        \"\"\" Training step. \"\"\"\n",
    "        return self._inner_step(batch, stage=\"train\")\n",
    "        \n",
    "    def validation_step(self, batch: dict[str: torch.Tensor], batch_idx: int):\n",
    "        \"\"\" Validation step. \"\"\"\n",
    "        return self._inner_step(batch, stage=\"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Configure the optimizers. \"\"\"\n",
    "        mode = \"max\"\n",
    "        monitor = \"val_acc\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(optimizer, \n",
    "                    mode=mode, factor=np.sqrt(0.1), patience=5, min_lr=0.5e-7),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": monitor,\n",
    "                \"frequency\": 10\n",
    "                # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
    "                # multiple of \"trainer.check_val_every_n_epoch\".\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=50, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lightning = lmodel(model, 4, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6968, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_lightning.model.parameters())[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | model      | Sequential         | 190 K \n",
      "1 | softmax    | Softmax            | 0     \n",
      "2 | train_acc  | MulticlassAccuracy | 0     \n",
      "3 | train_f1   | MulticlassF1Score  | 0     \n",
      "4 | val_acc    | MulticlassAccuracy | 0     \n",
      "5 | val_f1     | MulticlassF1Score  | 0     \n",
      "6 | val_auroc  | MulticlassAUROC    | 0     \n",
      "7 | test_acc   | MulticlassAccuracy | 0     \n",
      "8 | test_f1    | MulticlassF1Score  | 0     \n",
      "9 | test_auroc | MulticlassAUROC    | 0     \n",
      "--------------------------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.761     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   4%|▍         | 7/156 [00:00<00:05, 29.13it/s, v_num=3, train_loss_step=0.618, val_loss=0.655, val_acc=0.828, val_auroc=0.134, train_loss_epoch=0.611, train_acc=0.827]  "
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model_lightning, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
