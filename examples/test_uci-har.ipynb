{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCI_HARDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "            dataset_dir: str = None,\n",
    "            split: str = \"train\",\n",
    "            wsize: int = 10,\n",
    "            wstride: int = 1\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "            UCI-HAR dataset handler\n",
    "\n",
    "            Inputs:\n",
    "                dataset_dir: Directory of the prepare_har_dataset.py\n",
    "                    processed dataset.\n",
    "                wsize: window size\n",
    "                wstride: window stride\n",
    "        '''\n",
    "\n",
    "        self.wsize = wsize\n",
    "        self.wstride = wstride\n",
    "\n",
    "        # load dataset\n",
    "        files = filter(\n",
    "            lambda x: \"sensor.npy\" in x,\n",
    "            os.listdir(os.path.join(dataset_dir, \"UCI HAR Dataset\", split)))\n",
    "        \n",
    "        splits = [0]\n",
    "\n",
    "        STS = []\n",
    "        SCS = []\n",
    "        for f in files:\n",
    "            sensor_data = np.load(os.path.join(dataset_dir, \"UCI HAR Dataset\", split, f))\n",
    "            STS.append(sensor_data)\n",
    "            SCS.append(np.load(os.path.join(dataset_dir, \"UCI HAR Dataset\", split, f.replace(\"sensor\", \"class\"))))\n",
    "\n",
    "            splits.append(splits[-1] + sensor_data.shape[0])\n",
    "\n",
    "        self.splits = np.array(splits)\n",
    "\n",
    "        self.STS = np.concatenate(STS)\n",
    "        self.SCS = np.concatenate(SCS)\n",
    "\n",
    "        self.indices = np.arange(self.SCS.shape[0])\n",
    "        for i in range(wsize * wstride):\n",
    "            self.indices[self.splits[:-1] + i] = 0\n",
    "        self.indices = self.indices[np.nonzero(self.indices)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        first = self.indices[index]-self.wsize*self.wstride\n",
    "        last = self.indices[index]\n",
    "\n",
    "        return self.STS[first:last:self.wstride,:], self.SCS[first:last:self.wstride]\n",
    "    \n",
    "    def ts_from_array(self, indexes: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        assert len(indexes.shape) == 1 # only accept 1-dimensional arrays\n",
    "\n",
    "        return_sts = np.empty((indexes.shape[0], self.wsize, self.STS.shape[-1]))\n",
    "        return_scs = np.empty((indexes.shape[0], self.wsize))\n",
    "\n",
    "        for i, id in enumerate(indexes):\n",
    "            ts, c = self[id]\n",
    "            return_scs[i] = c\n",
    "            return_sts[i] = ts\n",
    "\n",
    "        return return_sts, return_scs\n",
    "    \n",
    "    def getWindowsIndex(self):\n",
    "\n",
    "        id = []\n",
    "        cl = []\n",
    "        for i, ix in enumerate(self.indices):\n",
    "            if np.unique(self.SCS[(ix-self.wsize*self.wstride):ix]).shape[0] == 1:\n",
    "                id.append(i)\n",
    "                cl.append(self.SCS[ix])\n",
    "        \n",
    "        return np.array(id), np.array(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = UCI_HARDataset(\"./datasets/UCI-HAR/\", split=\"train\", wsize=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wid, wcl = ds.getWindowsIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_barycenter(dataset, window_id, window_lb, n_random=100):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    selected = np.empty((np.unique(window_lb).shape[0], dataset.wsize, dataset.STS.shape[-1]))\n",
    "\n",
    "    for i, c in enumerate(np.unique(window_lb)):\n",
    "        # get the random windows for the class c\n",
    "\n",
    "        rw = np.random.choice(window_id[window_lb == c].reshape(-1), n_random)\n",
    "\n",
    "        ts, cs = dataset.ts_from_array(rw)\n",
    "\n",
    "        km = TimeSeriesKMeans(n_clusters=1, verbose=True, random_state=1, metric=\"dtw\", n_jobs=-1)\n",
    "        km.fit(ts)\n",
    "\n",
    "        selected[i] = km.cluster_centers_[0]\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barycenters = obtain_barycenter(ds, wid, wcl, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barycenters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1)\n",
    "[ax.plot(barycenters[0,:,d]) for d in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3ts.api.encodings import compute_DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCI_HARDataset_DM(UCI_HARDataset):\n",
    "\n",
    "    def __init__(self,\n",
    "            dataset_dir: str = None,\n",
    "            split: str = \"train\",\n",
    "            wsize: int = 10,\n",
    "            wstride: int = 1,\n",
    "            patterns: np.ndarray = None,\n",
    "            w: float = 0.2\n",
    "            ) -> None:\n",
    "        super().__init__(dataset_dir=dataset_dir, split=split, wsize=wsize, wstride=wstride)\n",
    "\n",
    "        '''\n",
    "            UCI-HAR dataset handler, with DF computation\n",
    "\n",
    "            Inputs:\n",
    "                dataset_dir: Directory of the prepare_har_dataset.py\n",
    "                    processed dataset.\n",
    "                wsize: window size\n",
    "                wstride: window stride\n",
    "                patterns: patterns used for DF computation\n",
    "                w: online dtw forgetting parameter\n",
    "        '''\n",
    "\n",
    "        assert patterns.shape[1] == wsize\n",
    "\n",
    "        self.rho = w\n",
    "\n",
    "        # compute and save DM to disk, one per split\n",
    "        if not os.path.exists(os.path.join(dataset_dir, \"cached_df\")):\n",
    "            os.mkdir(os.path.join(dataset_dir, \"cached_df\"))\n",
    "\n",
    "        self.cache_dir = os.path.join(dataset_dir, \"cached_df\")\n",
    "        self.split = split\n",
    "        self.patterns = patterns\n",
    "\n",
    "        for s in range(self.splits.shape[0] - 1):\n",
    "            save_path = os.path.join(dataset_dir, f\"cached_df/{split}_split{s}.npy\")\n",
    "            self.compute_dm_cache(patterns, self.splits[s:s+2], save_path)\n",
    "\n",
    "        self.loaded_split = None\n",
    "        self.temp = None\n",
    "\n",
    "    def compute_dm_cache(self, pattern, split, save_path):\n",
    "        DM = compute_DM(self.STS[split[0]:split[1]].T, pattern.transpose(0, 2, 1), rho=self.rho)\n",
    "\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            np.save(f, DM.transpose((0, 2, 1)))\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        id = self.indices[index]\n",
    "\n",
    "        # identify the split of the index\n",
    "\n",
    "        s = np.argwhere(self.splits > id)[0, 0] - 1\n",
    "        first = id - self.wsize*self.wstride - self.splits[s]\n",
    "        last = id - self.splits[s]\n",
    "\n",
    "        if self.loaded_split != s:\n",
    "            self.temp = np.load(os.path.join(self.cache_dir, f\"{self.split}_split{s}.npy\"))\n",
    "            self.loaded_split = s\n",
    "\n",
    "        return self.temp[:, first:last:self.wstride, :], self.STS[first:last:self.wstride, :], self.SCS[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = UCI_HARDataset_DM(\"./datasets/UCI-HAR/\", split=\"train\", wsize=64, patterns=barycenters, wstride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=time()\n",
    "for i in range(1000):\n",
    "    ds_df[i]\n",
    "    print(f\"\\r{i}: {(i+1)/(time()-a)} per/s\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from storage.label_mappings import UCI_HAR_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot, ax = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "ax_images = []\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        if i==0:\n",
    "            ax_images.append(ax[i, j].imshow(np.random.randn(64, 64), vmin=0, vmax=64))\n",
    "        if i==1:\n",
    "            ax_images.append(ax[i, j].imshow(np.random.randn(64, 64), vmin=0, vmax=32))\n",
    "\n",
    "        ax_twin = ax[i, j].twinx()\n",
    "        ax_twin.set_xlim([0, 64])\n",
    "        ax_twin.set_ylim([0, 64])\n",
    "        [ax_twin.plot(barycenters[i*3+j,:,h]*20 + 15, np.arange(64)) for h in range(6)]\n",
    "\n",
    "for i in range(10000, 20000, 16):\n",
    "    for col in range(3):\n",
    "        df, ts, c = ds_df[i]\n",
    "        for h in range(6):\n",
    "            ax_images[h].set_data(df[h].T)\n",
    "\n",
    "    display(plot)    \n",
    "    clear_output(wait = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
